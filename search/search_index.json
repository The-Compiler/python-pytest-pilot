{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pytest-pilot \u00b6 Slice in your test base thanks to powerful markers In pytest we can create custom markers and filter tests according to them using the -m flag, as explained here . However by default it only supports one kind of marker query behaviour: a test with a mark will run even if you do not use the -m <M> flag. If you wish to implement something more complex, you have to add code in your contest.py as explained here It is also not easy to understand what happens when a marker has a parameter (an argument) and how to filter according to this. It seems from the examples in the doc that the only way to handle these is again to add code in your contest.py In other words, it is not easy to expose a \"functional\" view to the user, even if all core mechanisms are perfectly working. pytest-pilot proposes a high-level API to create and register pytest markers so that they are easy to understand and use . To do this it does not use fancy mechanisms: it simply automates most the patterns demonstrated in the pytest documentation. Installing \u00b6 > pip install pytest-pilot Usage \u00b6 Basic \u00b6 The easiest way to define a marker is to create an instance of EasyMarker , anywhere in your code (in test files, in contest.py files, or even in other python files). For example let's create two markers: one envid marker defining the python environment on which we run. Tests that have this marker should run only if pytest is called with a --envid flag indicating that the environment is active. one flavour marker representing an optional filter. Tests that have this marker should run either if pytest is called with the correct --flavour , or if the --flavour flag is not set from pytest_pilot import EasyMarker flavourmarker = EasyMarker ( marker_id = 'flavour' , allowed_values = ( 'red' , 'yellow' )) envmarker = EasyMarker ( 'envid' , full_name = 'environment' , not_filtering_skips_marked = True ) We can now define a few tests in a test file: from .conftest import flavourmarker , envmarker @flavourmarker ( 'yellow' ) def test_yellow_noenv (): pass @flavourmarker ( 'yellow' ) @envmarker ( 'env1' ) def test_yellow_env1 (): pass @envmarker ( 'env2' ) def test_env2 (): pass @flavourmarker ( 'red' ) def test_red_noenv (): pass def test_nomark (): pass And we can see that the filtering works as expected: with no options, the tests marked as requiring an environment are correctly skipped: >>> pytest ============================= test session starts ============================= ( ... ) collected 5 items test_basic.py::test_yellow_noenv PASSED [ 20 % ] test_basic.py::test_yellow_env1 SKIPPED [ 40 % ] test_basic.py::test_env2 SKIPPED [ 60 % ] test_basic.py::test_red_noenv PASSED [ 80 % ] test_basic.py::test_nomark PASSED [ 100 % ] ===================== 3 passed, 2 skipped in 0 .09 seconds ===================== with an --envid option we can add one of them, the non marked tests still being able to run: >>> pytest --envid env1 ============================= test session starts ============================= ( ... ) collected 5 items test_basic.py::test_yellow_noenv PASSED [ 20 % ] test_basic.py::test_yellow_env1 PASSED [ 40 % ] test_basic.py::test_env2 SKIPPED [ 60 % ] test_basic.py::test_red_noenv PASSED [ 80 % ] test_basic.py::test_nomark PASSED [ 100 % ] ===================== 4 passed, 1 skipped in 0 .08 seconds ===================== we can use both options together and get expected results: >>> pytest --envid env2 --flavour red ============================= test session starts ============================= ( ... ) collected 5 items test_basic.py::test_yellow_noenv SKIPPED [ 20 % ] test_basic.py::test_yellow_env1 SKIPPED [ 40 % ] test_basic.py::test_env2 PASSED [ 60 % ] test_basic.py::test_red_noenv PASSED [ 80 % ] test_basic.py::test_nomark PASSED [ 100 % ] ===================== 3 passed, 2 skipped in 0 .07 seconds ===================== Verbosity levels \u00b6 You can use the verbose pytest flags to get a little more explanation about why tests are skipped or run: >>> pytest -vv --envid env2 ( verbose explanations ) >>> pytest -vvv --envid env2 ( even more verbose explanations ) Help \u00b6 Help on command options is automatically added to the pytest --help output: >>> pytest --help ( ... ) custom options: --flavour = NAME run tests marked as requiring flavour NAME ( marked with @flavour ( NAME )) , as well as tests not marked with @flavour. If you call ` pytest ` without this option, tests marked with @flavour will *all* be run --envid = NAME run tests marked as requiring environment NAME ( marked with @envid ( NAME )) , as well as tests not marked with @envid. Important: if you call ` pytest ` without this option, tests marked with @envid will *not* be run. ( ... ) Help on markers is automatically added to the pytest --markers output: >>> pytest --markers ( ... ) @pytest.mark.flavour ( value ) : mark test to run only when command option flavour is used to set --flavour to <value>, or if the option is not used at all. @pytest.mark.envid ( value ) : mark test to run only when command option environment is used to set --envid to <value>. ( ... ) Customization \u00b6 Almost everything is configurable from the EasyMarker constructor: help messages, command option short and long names, filtering behavious when the flag is present or not and the mark is present or not, etc. See API reference for details. Main features / benefits \u00b6 Create intuitive markers in minutes, with consistent behaviour and associated command options, documented with user-friendly help. See Also \u00b6 pytest tutorial on working with custom markers this excellent explanation about how to add options so as to filter on custom markers or on parameter names and values pytest hooks Others \u00b6 Do you like this library ? You might also like my other python libraries Want to contribute ? \u00b6 Details on the github page: https://github.com/smarie/python-pytest-pilot","title":"Home"},{"location":"#pytest-pilot","text":"Slice in your test base thanks to powerful markers In pytest we can create custom markers and filter tests according to them using the -m flag, as explained here . However by default it only supports one kind of marker query behaviour: a test with a mark will run even if you do not use the -m <M> flag. If you wish to implement something more complex, you have to add code in your contest.py as explained here It is also not easy to understand what happens when a marker has a parameter (an argument) and how to filter according to this. It seems from the examples in the doc that the only way to handle these is again to add code in your contest.py In other words, it is not easy to expose a \"functional\" view to the user, even if all core mechanisms are perfectly working. pytest-pilot proposes a high-level API to create and register pytest markers so that they are easy to understand and use . To do this it does not use fancy mechanisms: it simply automates most the patterns demonstrated in the pytest documentation.","title":"pytest-pilot"},{"location":"#installing","text":"> pip install pytest-pilot","title":"Installing"},{"location":"#usage","text":"","title":"Usage"},{"location":"#basic","text":"The easiest way to define a marker is to create an instance of EasyMarker , anywhere in your code (in test files, in contest.py files, or even in other python files). For example let's create two markers: one envid marker defining the python environment on which we run. Tests that have this marker should run only if pytest is called with a --envid flag indicating that the environment is active. one flavour marker representing an optional filter. Tests that have this marker should run either if pytest is called with the correct --flavour , or if the --flavour flag is not set from pytest_pilot import EasyMarker flavourmarker = EasyMarker ( marker_id = 'flavour' , allowed_values = ( 'red' , 'yellow' )) envmarker = EasyMarker ( 'envid' , full_name = 'environment' , not_filtering_skips_marked = True ) We can now define a few tests in a test file: from .conftest import flavourmarker , envmarker @flavourmarker ( 'yellow' ) def test_yellow_noenv (): pass @flavourmarker ( 'yellow' ) @envmarker ( 'env1' ) def test_yellow_env1 (): pass @envmarker ( 'env2' ) def test_env2 (): pass @flavourmarker ( 'red' ) def test_red_noenv (): pass def test_nomark (): pass And we can see that the filtering works as expected: with no options, the tests marked as requiring an environment are correctly skipped: >>> pytest ============================= test session starts ============================= ( ... ) collected 5 items test_basic.py::test_yellow_noenv PASSED [ 20 % ] test_basic.py::test_yellow_env1 SKIPPED [ 40 % ] test_basic.py::test_env2 SKIPPED [ 60 % ] test_basic.py::test_red_noenv PASSED [ 80 % ] test_basic.py::test_nomark PASSED [ 100 % ] ===================== 3 passed, 2 skipped in 0 .09 seconds ===================== with an --envid option we can add one of them, the non marked tests still being able to run: >>> pytest --envid env1 ============================= test session starts ============================= ( ... ) collected 5 items test_basic.py::test_yellow_noenv PASSED [ 20 % ] test_basic.py::test_yellow_env1 PASSED [ 40 % ] test_basic.py::test_env2 SKIPPED [ 60 % ] test_basic.py::test_red_noenv PASSED [ 80 % ] test_basic.py::test_nomark PASSED [ 100 % ] ===================== 4 passed, 1 skipped in 0 .08 seconds ===================== we can use both options together and get expected results: >>> pytest --envid env2 --flavour red ============================= test session starts ============================= ( ... ) collected 5 items test_basic.py::test_yellow_noenv SKIPPED [ 20 % ] test_basic.py::test_yellow_env1 SKIPPED [ 40 % ] test_basic.py::test_env2 PASSED [ 60 % ] test_basic.py::test_red_noenv PASSED [ 80 % ] test_basic.py::test_nomark PASSED [ 100 % ] ===================== 3 passed, 2 skipped in 0 .07 seconds =====================","title":"Basic"},{"location":"#verbosity-levels","text":"You can use the verbose pytest flags to get a little more explanation about why tests are skipped or run: >>> pytest -vv --envid env2 ( verbose explanations ) >>> pytest -vvv --envid env2 ( even more verbose explanations )","title":"Verbosity levels"},{"location":"#help","text":"Help on command options is automatically added to the pytest --help output: >>> pytest --help ( ... ) custom options: --flavour = NAME run tests marked as requiring flavour NAME ( marked with @flavour ( NAME )) , as well as tests not marked with @flavour. If you call ` pytest ` without this option, tests marked with @flavour will *all* be run --envid = NAME run tests marked as requiring environment NAME ( marked with @envid ( NAME )) , as well as tests not marked with @envid. Important: if you call ` pytest ` without this option, tests marked with @envid will *not* be run. ( ... ) Help on markers is automatically added to the pytest --markers output: >>> pytest --markers ( ... ) @pytest.mark.flavour ( value ) : mark test to run only when command option flavour is used to set --flavour to <value>, or if the option is not used at all. @pytest.mark.envid ( value ) : mark test to run only when command option environment is used to set --envid to <value>. ( ... )","title":"Help"},{"location":"#customization","text":"Almost everything is configurable from the EasyMarker constructor: help messages, command option short and long names, filtering behavious when the flag is present or not and the mark is present or not, etc. See API reference for details.","title":"Customization"},{"location":"#main-features-benefits","text":"Create intuitive markers in minutes, with consistent behaviour and associated command options, documented with user-friendly help.","title":"Main features / benefits"},{"location":"#see-also","text":"pytest tutorial on working with custom markers this excellent explanation about how to add options so as to filter on custom markers or on parameter names and values pytest hooks","title":"See Also"},{"location":"#others","text":"Do you like this library ? You might also like my other python libraries","title":"Others"},{"location":"#want-to-contribute","text":"Details on the github page: https://github.com/smarie/python-pytest-pilot","title":"Want to contribute ?"},{"location":"api_reference/","text":"API reference \u00b6 In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package. TODO","title":"API reference"},{"location":"api_reference/#api-reference","text":"In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package. TODO","title":"API reference"},{"location":"changelog/","text":"Changelog \u00b6 0.1.2 - packaging improvements \u00b6 packaging improvements: set the \"universal wheel\" flag to 1, and cleaned up the setup.py . In particular removed dependency to six for setup and added py.typed file, as well as set the zip_safe flag to False. Removed tests folder from package. Fixes #4 0.1.1 - pyproject.toml \u00b6 Added pyproject.toml 0.1.0 - First public version \u00b6 Initial fork from private repository","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#012-packaging-improvements","text":"packaging improvements: set the \"universal wheel\" flag to 1, and cleaned up the setup.py . In particular removed dependency to six for setup and added py.typed file, as well as set the zip_safe flag to False. Removed tests folder from package. Fixes #4","title":"0.1.2 - packaging improvements"},{"location":"changelog/#011-pyprojecttoml","text":"Added pyproject.toml","title":"0.1.1 - pyproject.toml"},{"location":"changelog/#010-first-public-version","text":"Initial fork from private repository","title":"0.1.0 - First public version"},{"location":"long_description/","text":"pytest-pilot \u00b6 Slice in your test base thanks to powerful markers The documentation for users is available here: https://smarie.github.io/python-pytest-pilot/ A readme for developers is available here: https://github.com/smarie/python-pytest-pilot","title":"pytest-pilot"},{"location":"long_description/#pytest-pilot","text":"Slice in your test base thanks to powerful markers The documentation for users is available here: https://smarie.github.io/python-pytest-pilot/ A readme for developers is available here: https://github.com/smarie/python-pytest-pilot","title":"pytest-pilot"}]}